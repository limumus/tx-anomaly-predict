import os
from datetime import datetime

import requests
import json
from dotenv import load_dotenv
from collections import deque
from typing import List, Dict, Any, Optional
from tqdm import tqdm

# LangChainç›¸å…³æ¨¡å—
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_chroma import Chroma
from langchain.embeddings.base import Embeddings
from langchain_core.runnables import Runnable,RunnableLambda
from langchain_core.output_parsers import StrOutputParser

# ============== ç³»ç»Ÿé…ç½® ==============
CONFIG = {
    "JSON_FOLDER_PATH": "output_de",
    "PERSIST_DIR": "vector_store_realtime_0",
    "INTERMEDIATE_FOLDER": "intermediate_data",  # æ–°å¢ä¿å­˜ä¸­é—´æ•°æ®çš„æ–‡ä»¶å¤¹

    "HTTP_PROXY": "http://127.0.0.1:65151",
    "HTTPS_PROXY": "http://127.0.0.1:65151",

}

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ============== ç™¾åº¦æ–‡å¿ƒERNIEæ¨¡å‹å°è£… ==============
class BaiduErnieAI:
    def __init__(self):
        self.access_key = CONFIG["BAIDU_ACCESS_KEY"]
        self.secret_key = CONFIG["BAIDU_SECRET_KEY"]
        self.api_url = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/ernie-speed-128k"

    def get_access_token(self):
        url = f"https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={self.access_key}&client_secret={self.secret_key}"
        response = requests.post(url, headers={'Content-Type': 'application/json'})
        return response.json().get("access_token")

    def generate_response(self, messages: List[Dict[str, str]]) -> str:
        """ä¿®æ­£åçš„APIè°ƒç”¨æ–¹æ³•ï¼Œåªæ¥å—æ¶ˆæ¯åˆ—è¡¨"""
        try:
            access_token = self.get_access_token()
            url = f"{self.api_url}?access_token={access_token}"

            payload = json.dumps({"messages": messages})
            response = requests.post(url,
                                     headers={'Content-Type': 'application/json'},
                                     data=payload)
            response_data = response.json()

            if "error_code" in response_data:
                print(f"APIé”™è¯¯: {response_data.get('error_msg')}")
                return f"APIé”™è¯¯: {response_data.get('error_msg')}"
            return response_data.get("result", "æœªèƒ½è·å–å›ç­”")

        except Exception as e:
            print(f"APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return "APIè°ƒç”¨å¼‚å¸¸"

    def simple_query(self, query: str, context: str = None) -> str:
        """æ–°å¢çš„ç®€åŒ–æŸ¥è¯¢æ–¹æ³•ï¼Œç”¨äºæµ‹è¯•"""
        messages = []
        if context:
            messages.append({"role": "user", "content": context})
        messages.append({"role": "user", "content": query})
        return self.generate_response(messages)


# ============== åµŒå…¥æœåŠ¡å°è£… ==============
class JinaEmbedding(Embeddings):
    """å®æ—¶åµŒå…¥æœåŠ¡"""

    def __init__(self):
        self.api_url = "https://api.jina.ai/v1/embeddings"
        self.headers = {
            "Authorization": f"Bearer {CONFIG['JINA_API_KEY']}",
            "Content-Type": "application/json"
        }

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self._get_embedding(text) for text in texts]

    def embed_query(self, text: str) -> List[float]:
        return self._get_embedding(text)

    def _get_embedding(self, text: str) -> List[float]:
        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json={
                    "model": CONFIG["EMBEDDING_MODEL"],
                    "input": [text[:8192]]  # æˆªæ–­è¶…é•¿æ–‡æœ¬
                },
                timeout=15
            )
            response.raise_for_status()
            return response.json()['data'][0]['embedding']
        except Exception as e:
            print(f"âš ï¸ åµŒå…¥å¤±è´¥: {str(e)}")
            return []


# ============== ERNIEæ¨¡å‹åŒ…è£…å™¨ ==============
class ErnieLLMWrapper(Runnable):
    """å…¼å®¹LangChain Runnableæ¥å£çš„ERNIEæ¨¡å‹åŒ…è£…å™¨"""

    def __init__(self, ernie_ai: BaiduErnieAI):
        super().__init__()
        self.ernie_ai = ernie_ai

    def invoke(self, input_data: Any, config: Optional[Dict] = None) -> AIMessage:
        """å¤„ç†å•æ¬¡è°ƒç”¨ï¼Œå…¼å®¹å¤šç§è¾“å…¥ç±»å‹"""
        try:
            # å¤„ç†ä¸åŒç±»å‹çš„è¾“å…¥
            if isinstance(input_data, dict):
                # å­—å…¸è¾“å…¥
                query = input_data.get("input", "")
                reference = input_data.get("reference_text", "")
            elif hasattr(input_data, "messages"):
                # ChatPromptValueè¾“å…¥
                messages = input_data.messages
                query = next((msg.content for msg in messages if isinstance(msg, HumanMessage)), "")
                reference = next((msg.content for msg in messages if isinstance(msg, SystemMessage)), "")
            else:
                return AIMessage(content="æ— æ•ˆçš„è¾“å…¥æ ¼å¼")

            # æ„å»ºERNIE APIéœ€è¦çš„æ¶ˆæ¯æ ¼å¼
            messages = []
            if reference:
                messages.append({"role": "user", "content": f"å‚è€ƒå†…å®¹: {reference[:1000]}"})
            messages.append({"role": "user", "content": query})

            # è°ƒç”¨ERNIE API
            response = self.ernie_ai.generate_response(messages)
            return AIMessage(content=response)

        except Exception as e:
            print(f"è°ƒç”¨ERNIE APIå‡ºé”™: {str(e)}")
            return AIMessage(content=f"å¤„ç†é”™è¯¯: {str(e)}")

    async def ainvoke(self, input_data: Any, config: Optional[Dict] = None) -> AIMessage:
        """å¼‚æ­¥è°ƒç”¨"""
        return self.invoke(input_data, config)

    def stream(self, input_data: Any, config: Optional[Dict] = None):
        """æµå¼å¤„ç†"""
        result = self.invoke(input_data, config)
        yield result

    async def astream(self, input_data: Any, config: Optional[Dict] = None):
        """å¼‚æ­¥æµå¼å¤„ç†"""
        async for chunk in self.stream(input_data, config):
            yield chunk

# ============== å‘é‡æ•°æ®åº“ç®¡ç† ==============
def load_or_create_vector_store(persist_dir: str, embedding: Embeddings) -> Chroma:
    """åŠ è½½æˆ–åˆ›å»ºå‘é‡æ•°æ®åº“"""
    if os.path.exists(persist_dir):
        print(f"ğŸ” åŠ è½½å·²æœ‰å‘é‡æ•°æ®åº“: {persist_dir}")
        return Chroma(persist_directory=persist_dir, embedding_function=embedding)
    else:
        print(f"ğŸ†• åˆ›å»ºæ–°å‘é‡æ•°æ®åº“: {persist_dir}")
        return Chroma(persist_directory=persist_dir, embedding_function=embedding)


# ============== å¯¹è¯é“¾æ„å»º ==============
'''
def create_conversational_chain(vector_store: Chroma, llm: Runnable) -> Runnable:
    """åˆ›å»ºå¸¦æ–‡æ¡£æ¯”å¯¹çš„å¯¹è¯å¼æ£€ç´¢é“¾"""

    # 1. æ”¹è¿›çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥æ£€ç´¢å™¨prompt
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªé—®é¢˜ä¼˜åŒ–åŠ©æ‰‹ï¼Œè¯·æ ¹æ®å¯¹è¯å†å²å’Œæä¾›çš„å‚è€ƒæ–‡æ¡£ä¼˜åŒ–é—®é¢˜ã€‚

        å‚è€ƒæ–‡æ¡£ç‰‡æ®µ:
        {reference_text}

        å¯¹è¯å†å²:
        {chat_history}

        åŸå§‹é—®é¢˜: {input}

        è¯·è¾“å‡ºä¼˜åŒ–åçš„é—®é¢˜:""")
    ])

    # 2. æ”¹è¿›çš„é—®ç­”prompt
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åˆçº¦åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:

        ç›¸å…³æ–‡æ¡£å†…å®¹:
        {context}

        å‚è€ƒæ¯”å¯¹æ–‡æœ¬:
        {reference_text}

        """),

        MessagesPlaceholder("chat_history"),
        ("user", "{input}")
    ])
    retriever = vector_store.as_retriever()

    # 3. åˆ›å»ºå¸¦æ–‡æ¡£æ¯”å¯¹çš„æ£€ç´¢é“¾
    def add_reference_text(input_dict: Dict):
        """ä»è¾“å…¥ä¸­æå–å‚è€ƒæ–‡æœ¬"""
        # è¿™é‡Œå¯ä»¥ä»è¾“å…¥ä¸­æå–æˆ–ç”Ÿæˆå‚è€ƒæ–‡æœ¬
        # ç¤ºä¾‹ï¼šç®€å•ä½¿ç”¨å‰300å­—ç¬¦ä½œä¸ºå‚è€ƒ
        reference = input_dict.get("input", "")[:300]
        return {**input_dict, "reference_text": reference}

    # ä¿®æ”¹åçš„å®Œæ•´é“¾æ¡
    retrieval_chain = (
            RunnableLambda(add_reference_text)
            | {
                "input": lambda x: x["input"],
                "chat_history": lambda x: x.get("chat_history", []),
                "reference_text": lambda x: x["reference_text"]
            }
            | create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    )

    # åŒ…è£…æœ€ç»ˆç»“æœä¸ºå­—å…¸æ ¼å¼
    full_chain = (
            {
                "input": lambda x: x["input"],
                "chat_history": lambda x: x.get("chat_history", []),
                "context": retrieval_chain,
                "reference_text": lambda x: x.get("reference_text", "")
            }
            | create_stuff_documents_chain(llm, qa_prompt)
            | {
                "answer": lambda x: x,
                "context": lambda x: x  # è¿™é‡Œéœ€è¦ä»retrieval_chainè·å–ä¸Šä¸‹æ–‡
            }
    )

    return full_chain
'''

def create_conversational_chain(vector_store: Chroma, llm: Runnable) -> Runnable:
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("human", """è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æœ¬ä¼˜åŒ–é—®é¢˜ï¼š
        å‚è€ƒæ–‡æœ¬ï¼š{reference_text}
        åŸå§‹é—®é¢˜ï¼š{input}
        ä¼˜åŒ–åçš„é—®é¢˜ï¼š""")
    ])
    # é—®ç­”prompt
    qa_prompt = ChatPromptTemplate.from_messages([
        ("human", """è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
        ç›¸å…³ä¸Šä¸‹æ–‡ï¼š
        {context}
        å‚è€ƒæ–‡æœ¬ï¼š
        {reference_text}
        é—®é¢˜ï¼š
        {input}""")
    ])

    retriever = vector_store.as_retriever(search_kwargs={"k": 3})
    # å†å²æ„ŸçŸ¥æ£€ç´¢
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    # æ–‡æ¡£å¤„ç†
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    return create_retrieval_chain(history_aware_retriever, question_answer_chain)
# ============== ç»“æœå¤„ç†å·¥å…· ==============
class ResultSaver:
    def __init__(self, output_dir: str = "results"):
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

    def save_result(self, query: str, result: Dict, source_file: str):
        """æ”¹è¿›çš„ç»“æœä¿å­˜æ–¹æ³•"""
        # è°ƒè¯•è¾“å‡º
        print("å®Œæ•´ç»“æœç»“æ„:", json.dumps(result, indent=2, ensure_ascii=False))

        # æ£€æŸ¥æœ‰æ•ˆç­”æ¡ˆçš„æ¡ä»¶
        is_valid = (
                isinstance(result, dict) and
                result.get("answer") and
                not result["answer"].startswith(("æœªèƒ½è·å–", "APIè°ƒç”¨å¼‚å¸¸"))
        )

        if is_valid:
            filename = os.path.join(
                self.output_dir,
                f"result_{os.path.basename(source_file)}"
            )
            data = {
                "query": query,
                "answer": result["answer"],
                "source": source_file,
                "context": [
                    {
                        "content": doc.page_content[:200] + "...",
                        "source": doc.metadata.get("source_file", "unknown")
                    }
                    for doc in result.get("context", [])
                ],
                "timestamp": datetime.now().isoformat()
            }
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            return True
        return False

# ============== ä¸»æ‰§è¡Œæµç¨‹ ==============
if __name__ == "__main__":

    embedding = JinaEmbedding()
    vector_store = load_or_create_vector_store(CONFIG["PERSIST_DIR"], embedding)
    ernie_ai = BaiduErnieAI()
    llm = ErnieLLMWrapper(ernie_ai)

    # æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§
    print("=== æœåŠ¡æ£€æŸ¥ ===")
    print(f"å‘é‡æ•°æ®åº“è®°å½•: {vector_store._collection.count()}")
    #print(f"ERNIEæµ‹è¯•: {ernie_ai.simple_query('æµ‹è¯•', 'æµ‹è¯•ä¸Šä¸‹æ–‡')}")

    conversational_chain = create_conversational_chain(vector_store, llm)

    # 6. ç¤ºä¾‹å¯¹è¯
    chat_history = []
    query = "Vector databases contain only attack contracts. Please analyze the current input in the context of the preceding and following text to determine whether it pertains to attack contracts?"
    system_input = r"G:\safe\pythonProject\output\output\2018-04\BEC_exp.sol\0xad89ff16fd1ebe3a0a7cf4ed282302c06626c1af33221ebe0d3a470aba4a660f.txt.json"
    print("\n=== å¼€å§‹æµ‹è¯•æµç¨‹ ===")

    result = conversational_chain.invoke({
        "input": query,
        "chat_history": [],
        "reference_text": json.dumps(system_input, ensure_ascii=False)[:1000]  # ç¡®ä¿å‚è€ƒæ–‡æœ¬ä¼ å…¥
    })

    # å¤„ç†ç»“æœ
    print("\n=== æŸ¥è¯¢ç»“æœ ===")
    if isinstance(result, dict) and "answer" in result:
        print(f"ç­”æ¡ˆ: {result['answer']}")
        if "context" in result:
            print("\nå‚è€ƒæ–‡æ¡£:")
            for doc in result["context"]:
                print(f"- {doc.metadata.get('source_file', 'æœªçŸ¥æ¥æº')}")
    else:
        print(f"åŸå§‹ç»“æœ: {result}")

    '''
    # è°ƒè¯•è¾“å‡º
    print("\n=== è°ƒè¯•ä¿¡æ¯ ===")
    print("1. å‘é‡æ•°æ®åº“æ£€ç´¢æµ‹è¯•:")
    print(vector_store.similarity_search(test_query, k=1)[0].metadata)

    print("\n2. ERNIE APIæµ‹è¯•:")
    print(ernie_ai.generate_response(test_query, "æµ‹è¯•ä¸Šä¸‹æ–‡"))

    print("\n3. å®Œæ•´é“¾è·¯æµ‹è¯•ç»“æœ:")
    print(test_result)

    # æ‰¹é‡å¤„ç†
    #print("\n=== å¼€å§‹æ‰¹é‡å¤„ç† ===")
    #processor = BatchProcessor(
    #    base_dir=r"G:\safe\pythonProject\output\output",
    #    result_dir="processed_results"
    #)
    #processor.process_all(query)

    # åˆå§‹åŒ–ç»“æœä¿å­˜å™¨
    #result_saver = ResultSaver()

    # éå†å¤„ç†æ‰€æœ‰JSONæ–‡ä»¶
    base_dir = r"G:\safe\pythonProject\output\output"
    total_files = 0
    processed_files = 0

    print(f"å¼€å§‹å¤„ç†ç›®å½•: {base_dir}")

    for root, _, files in os.walk(base_dir):
        for file in files:
            if file.endswith(".txt.json"):
                total_files += 1
                file_path = os.path.join(root, file)

                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        reference_text = json.load(f)

                    # æ‰§è¡ŒæŸ¥è¯¢
                    result = conversational_chain.invoke({
                        "input": query,
                        "chat_history": [],
                        "reference_text": json.dumps(reference_text, ensure_ascii=False)[:1000]
                    })

                    # ä¿å­˜æœ‰ç­”æ¡ˆçš„ç»“æœ
                    if result_saver.save_result(query, result, file_path):
                        processed_files += 1
                        print(f"âœ… å·²ä¿å­˜ç»“æœ: {file_path}")
                    else:
                        print(f"âš ï¸ æ— æœ‰æ•ˆç­”æ¡ˆ: {file_path}")

                except Exception as e:
                    print(f"âŒ å¤„ç†å¤±è´¥ {file_path}: {str(e)}")

    print(f"\nå¤„ç†å®Œæˆ! å…±å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­ {processed_files} ä¸ªåŒ…å«æœ‰æ•ˆç­”æ¡ˆ")
    print(f"ç»“æœä¿å­˜è‡³: {result_saver.output_dir}")

    # æ‰§è¡ŒæŸ¥è¯¢
    result = conversational_chain.invoke({
        "input": query,
        "chat_history": chat_history,
        "reference_text": json.dumps(reference_text, ensure_ascii=False)[:1000]  # é™åˆ¶é•¿åº¦
    })


    # å¤„ç†ç»“æœè¾“å‡º
    if isinstance(result, dict):
        print(f"âœ… ç­”æ¡ˆ: {result.get('answer', 'æ— ç­”æ¡ˆ')}")
        if "context" in result:
            print("\næ¥æºæ–‡æ¡£:")
            if isinstance(result["context"], list):
                for doc in result["context"]:
                    print(f"- {doc.metadata.get('source_file', 'æœªçŸ¥æ¥æº')}")
            else:
                print("- æ— ä¸Šä¸‹æ–‡æ–‡æ¡£")
    else:
        print(f"âœ… ç­”æ¡ˆ: {result}")
'''