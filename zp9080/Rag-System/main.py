# å®‰è£…ä¾èµ–ï¼ˆéœ€æå‰å®‰è£…ï¼‰
# pip install haystack-ai python-dotxfa sentence-transformers

import os
import json
from dotenv import load_dotenv
from haystack import Pipeline
from haystack.components.builders import PromptBuilder
from haystack.components.generators import OpenAIGenerator
from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder
from haystack.components.writers import DocumentWriter
from haystack_integrations.document_stores.chroma import ChromaDocumentStore
from haystack_integrations.components.retrievers.chroma import ChromaEmbeddingRetriever
from haystack import Document
from haystack.utils import Secret
from typing import List, Dict, Any
from collections import deque
from tqdm import tqdm


# 1. è‡ªå®šä¹‰JSONæ–‡æ¡£å¤„ç†å™¨
class JSONDocumentProcessor:       
    @staticmethod
    def process_node(calls):
        queue = deque()

        for call in calls:
            queue.append(call)
        cont=[]
        all_slices = []
        count=0
        while queue:
           
            for i in range(len(queue)):
                node = queue.popleft()  # å…³é”®æ“ä½œï¼šå…ˆè¿›å…ˆå‡º
                cont.append(
                    {
                        "contract":node.get("contract"),
                        "function":node.get("function"),
                        "args":node.get("args"),
                        "original_code":node.get("original_code"),
                        "description":node.get("description"),
                        "return_value":node.get("return_value"),
                        "depth":node.get("depth")
                    }
                )
                count+=1
                if(count%10==0):
                    #all_sliceså½“å‰æ˜¯ä¸ªåˆ—è¡¨ï¼Œå…ƒç´ ä¹Ÿæ˜¯åˆ—è¡¨ï¼Œå…ƒç´ è¿™ä¸ªåˆ—è¡¨ä¸­å­˜çš„æ˜¯å­—å…¸ç±»å‹
                    all_slices.append(cont)
                    cont=[]

                internal_calls=node.get("internal_calls")
                if internal_calls:
                    for call in internal_calls:
                        queue.append(call)
                
        if(cont!=[]): all_slices.append(cont)

        return all_slices

    @staticmethod
    def load_json_data(folder_path: str) -> List[Document]:
        """å¢å¼ºç‰ˆJSONå¤„ç†å™¨ï¼Œæ”¯æŒæ·±åº¦æ„ŸçŸ¥åˆ‡ç‰‡"""
        documents = []
        index = 0
        
        # è·å–æ‰€æœ‰æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå‡†ç¡®ç»Ÿè®¡æ€»æ•°ï¼‰
        file_paths = []
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                file_paths.append(os.path.join(root, file))
        
        # ä¸»è¿›åº¦æ¡ï¼šæ–‡ä»¶å¤„ç†è¿›åº¦[6](@ref)
        with tqdm(file_paths, desc="ğŸ“ Processing files", unit="file") as file_pbar:
            for file_path in file_pbar:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # è®¾ç½®å½“å‰æ–‡ä»¶åçš„åŠ¨æ€æ˜¾ç¤º[4](@ref)
                    file_pbar.set_postfix(file=os.path.basename(file_path)[:15])
                    
                    calls = data.get("logs", {}).get("calls", [])
                    all_slices = JSONDocumentProcessor.process_node(calls)
                    
                    # å­è¿›åº¦æ¡ï¼šåˆ‡ç‰‡ç”Ÿæˆè¿›åº¦[3,5](@ref)
                    with tqdm(all_slices, desc="ğŸ”§ Generating slices", leave=False, unit="slice") as slice_pbar:
                        for item in slice_pbar:
                            content = json.dumps(item, ensure_ascii=False)
                            meta = {"slice_id": index+1, "source_file": file_path}
                            index += 1
                            documents.append(Document(content=content, meta=meta))
                            
                except Exception as e:
                    tqdm.write(f"âš ï¸ Error processing {file_path}: {str(e)}")
        
        return documents


# é…ç½®è·¯å¾„
JSON_FOLDER_PATH = "/home/zp9080/Code/tx-anomaly-predict/hst/output_1"
PERSIST_DIR = "/home/zp9080/Code/tx-anomaly-predict/zp9080/Rag-System/vector_store"
MODEL_PATH = "/home/zp9080/models/all-mpnet-base-v2"

# åˆå§‹åŒ–Chromaæ–‡æ¡£å­˜å‚¨
document_store = ChromaDocumentStore(
    persist_path=PERSIST_DIR,
)

# ç´¢å¼•ç®¡é“
indexing_pipeline = Pipeline()
indexing_pipeline.add_component(
    "doc_embedder",
    SentenceTransformersDocumentEmbedder(model=MODEL_PATH)
)
indexing_pipeline.add_component(
    "writer",
    DocumentWriter(document_store=document_store)
)
indexing_pipeline.connect("doc_embedder", "writer")

# RAGæŸ¥è¯¢ç®¡é“
rag_pipeline = Pipeline()
rag_pipeline.add_component(
    "text_embedder",
    SentenceTransformersTextEmbedder(model=MODEL_PATH)
)
rag_pipeline.add_component(
    "retriever",
    ChromaEmbeddingRetriever(document_store) # ä½¿ç”¨Chromaå†…ç½®çš„æ£€ç´¢å™¨
)
rag_pipeline.add_component(
    "prompt_builder",
    PromptBuilder(template="""
        æ ¹æ®ä»¥ä¸‹åˆçº¦è°ƒç”¨ä¿¡æ¯å›ç­”é—®é¢˜ï¼š
        {% for document in documents %}
        è°ƒç”¨é“¾ç‰‡æ®µ{{ document.meta.slice_id }}: 
        {{ document.content|truncate(300) }}
        {% endfor %}
        é—®é¢˜ï¼š{{ query }}
        ç­”æ¡ˆï¼š
    """)
)
rag_pipeline.add_component("llm", OpenAIGenerator(
    model="deepseek/deepseek-r1/community",
    api_base_url="https://api.ppinfra.com/v3/openai",
    generation_kwargs={"temperature": 0.3},
    api_key=Secret.from_token("sk_eevcJPk1gMUEMlBeAaG4Qj-jBP2kEN3N5FYXrkrzEfI")
))

# è¿æ¥ç®¡é“ç»„ä»¶
rag_pipeline.connect("text_embedder", "retriever")
rag_pipeline.connect("retriever", "prompt_builder.documents")
rag_pipeline.connect("prompt_builder", "llm")

if __name__ == "__main__":
    # é¦–æ¬¡è¿è¡Œæˆ–éœ€è¦æ›´æ–°ç´¢å¼•æ—¶æ‰§è¡Œ
    if not os.path.exists(PERSIST_DIR):
        print("æ­£åœ¨æ„å»ºå‘é‡æ•°æ®åº“...")
        documents = JSONDocumentProcessor.load_json_data(JSON_FOLDER_PATH)
        indexing_pipeline.run({"doc_embedder": {"documents": documents}})
        document_store.persist()

    # æ‰§è¡ŒæŸ¥è¯¢
    query = "è¯·åˆ†æåˆçº¦è°ƒç”¨æ¨¡å¼ä¸­çš„å¼‚å¸¸è¡Œä¸º"
    result = rag_pipeline.run({
        "text_embedder": {"text": query},
        "prompt_builder": {"query": query}
    })
    
    print("ç­”æ¡ˆï¼š", result["llm"]["replies"][0])