
import json
from typing import List, Dict, Any
from haystack import Document
from collections import deque
from transformers import AutoTokenizer
from tqdm import tqdm
import tiktoken
import os

# 1. è‡ªå®šä¹‰JSONæ–‡æ¡£å¤„ç†å™¨
class JSONDocumentProcessor:       
    @staticmethod
    def process_node(calls):
        queue = deque()

        for call in calls:
            queue.append(call)
        cont=[]
        all_slices = []
        count=0
        while queue:
           
            for i in range(len(queue)):
                node = queue.popleft()  # å…³é”®æ“ä½œï¼šå…ˆè¿›å…ˆå‡º
                cont.append(
                    {
                        "contract":node.get("contract"),
                        "function":node.get("function"),
                        "args":node.get("args"),
                        "original_code":node.get("original_code"),
                        "description":node.get("description"),
                        "return_value":node.get("return_value"),
                        "depth":node.get("depth")
                    }
                )
                count+=1
                if(count%10==0):
                    #all_sliceså½“å‰æ˜¯ä¸ªåˆ—è¡¨ï¼Œå…ƒç´ ä¹Ÿæ˜¯åˆ—è¡¨ï¼Œå…ƒç´ è¿™ä¸ªåˆ—è¡¨ä¸­å­˜çš„æ˜¯å­—å…¸ç±»å‹
                    all_slices.append(cont)
                    cont=[]

                internal_calls=node.get("internal_calls")
                if internal_calls:
                    for call in internal_calls:
                        queue.append(call)
                
        if(cont!=[]): all_slices.append(cont)

        return all_slices

    @staticmethod
    def load_json_data(folder_path: str) -> List[Document]:
        """å¢å¼ºç‰ˆJSONå¤„ç†å™¨ï¼Œæ”¯æŒæ·±åº¦æ„ŸçŸ¥åˆ‡ç‰‡"""
        documents = []
        index = 0
        
        # è·å–æ‰€æœ‰æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºå‡†ç¡®ç»Ÿè®¡æ€»æ•°ï¼‰
        file_paths = []
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                file_paths.append(os.path.join(root, file))
        
        # ä¸»è¿›åº¦æ¡ï¼šæ–‡ä»¶å¤„ç†è¿›åº¦[6](@ref)
        with tqdm(file_paths, desc="ğŸ“ Processing files", unit="file") as file_pbar:
            for file_path in file_pbar:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # è®¾ç½®å½“å‰æ–‡ä»¶åçš„åŠ¨æ€æ˜¾ç¤º[4](@ref)
                    file_pbar.set_postfix(file=os.path.basename(file_path)[:15])
                    
                    calls = data.get("logs", {}).get("calls", [])
                    all_slices = JSONDocumentProcessor.process_node(calls)
                    
                    # å­è¿›åº¦æ¡ï¼šåˆ‡ç‰‡ç”Ÿæˆè¿›åº¦[3,5](@ref)
                    with tqdm(all_slices, desc="ğŸ”§ Generating slices", leave=False, unit="slice") as slice_pbar:
                        for item in slice_pbar:
                            content = json.dumps(item, ensure_ascii=False)
                            meta = {"slice_id": index+1, "source_file": file_path}
                            index += 1
                            documents.append(Document(content=content, meta=meta))
                            
                except Exception as e:
                    tqdm.write(f"âš ï¸ Error processing {file_path}: {str(e)}")
        
        return documents


def count_tokens_with_tiktoken(text: str, model_name: str = "gpt-3.5-turbo") -> int:
    """ä½¿ç”¨ä¸æ¨¡å‹åŒ¹é…çš„tokenizerè®¡ç®—æ–‡æœ¬tokenæ•°"""
    try:
        # è·å–å¯¹åº”æ¨¡å‹çš„ç¼–ç å™¨ï¼ˆcl100k_baseé€‚ç”¨äºGPT-3.5/4ï¼‰
        encoding = tiktoken.encoding_for_model(model_name)
        return len(encoding.encode(text))
    except KeyError:
        # å…¼å®¹æ—§ç‰ˆæ¨¡å‹åç§°
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))

from sentence_transformers import SentenceTransformer

MODEL_PATH = "/home/zp9080/models/all-mpnet-base-v2"

# åŠ è½½æ¨¡å‹
model = SentenceTransformer(MODEL_PATH)

# è·å–æœ€å¤§ token é•¿åº¦
max_seq_length = model.get_max_seq_length()
print(f"æ¨¡å‹æœ€å¤§æ”¯æŒ token æ•°é‡: {max_seq_length}")


# JSON_FOLDER_PATH = "/home/zp9080/Code/tx-anomaly-predict/hst/output_1"
# documents = JSONDocumentProcessor.load_json_data(JSON_FOLDER_PATH)

# # ä¸ºæ¯ä¸ªæ–‡æ¡£è®¡ç®—tokenæ•°
# token_counts = []
# for idx, document in enumerate(documents, start=1):
#     if not hasattr(document, 'content'):
#         print(f"æ–‡æ¡£{idx}ç¼ºå°‘contentå±æ€§ï¼Œå·²è·³è¿‡")
#         continue
        
#     text_content = document.content
#     if not isinstance(text_content, str) or len(text_content.strip()) == 0:
#         print(f"æ–‡æ¡£{idx}çš„contentä¸ºç©ºæˆ–éå­—ç¬¦ä¸²ç±»å‹ï¼Œå·²è·³è¿‡")
#         continue

#     try:
#         token_count = count_tokens_with_tiktoken(text_content)
#         token_counts.append(token_count)
#         # print(f"æ–‡æ¡£{idx}çš„tokenæ•°é‡: {token_count}")
#     except Exception as e:
#         print(f"æ–‡æ¡£{idx}å¤„ç†å¤±è´¥: {str(e)}")

# # åœ¨ç°æœ‰ä»£ç æœ«å°¾æ·»åŠ æœ€å¤§tokenç»Ÿè®¡é€»è¾‘
# if token_counts:
#     max_tokens = max(token_counts)
#     print(f"æœ€å¤§tokenæ•°é‡: {max_tokens}")
#     print(f"å¯¹åº”æ–‡æ¡£ç¼–å·: {token_counts.index(max_tokens)+1}")  # æ˜¾ç¤ºç¬¬ä¸€ä¸ªå‡ºç°æœ€å¤§å€¼çš„æ–‡æ¡£åºå·
# else:
#     print("æ— æœ‰æ•ˆtokenæ•°æ®å¯è®¡ç®—æœ€å¤§å€¼")
# # æ‰“å°ç»Ÿè®¡ç»“æœ
# print(f"\næ€»æ–‡æ¡£æ•°: {len(documents)}")
# print(f"æœ‰æ•ˆè®¡ç®—æ–‡æ¡£æ•°: {len(token_counts)}")
# print(f"å¹³å‡tokenæ•°: {sum(token_counts)/len(token_counts):.1f}" if token_counts else "æ— æœ‰æ•ˆæ•°æ®")

# # åœ¨ç°æœ‰ç»Ÿè®¡ä»£ç åæ·»åŠ ä»¥ä¸‹å†…å®¹
# threshold = 30526
# over_limit_docs = []

# # éå†æ‰€æœ‰æ–‡æ¡£å’Œtokenè®¡æ•°
# for idx, (doc, token_count) in enumerate(zip(documents, token_counts), start=1):
#     if token_count > threshold:
#         # è·å–æ¥æºæ–‡ä»¶åï¼ˆä»å…ƒæ•°æ®ä¸­æå–ï¼‰
#         source_file = doc.meta.get("source_file", "æœªçŸ¥æ–‡ä»¶") 
#         # è®°å½•æ–‡æ¡£ä¿¡æ¯
#         over_limit_docs.append({
#             "æ–‡æ¡£ç¼–å·": idx,
#             "tokenæ•°é‡": token_count,
#             "æ¥æºæ–‡ä»¶": os.path.basename(source_file)[:30] + ("..." if len(source_file)>30 else "")
#         })

# # è¾“å‡ºç»Ÿè®¡ç»“æœ
# if over_limit_docs:
#     print("\n=== è¶…é™æ–‡æ¡£ç»Ÿè®¡ ===")
#     print(f"é˜ˆå€¼è®¾å®š: {threshold} tokens")
#     print(f"è¶…é™æ–‡æ¡£æ€»æ•°: {len(over_limit_docs)}")
#     print(f"æ€»è®¡tokenæ•°é‡: {sum(d['tokenæ•°é‡'] for d in over_limit_docs)}")
    
#     # æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼ˆé™åˆ¶æœ€å¤šæ˜¾ç¤º10æ¡ï¼‰
#     print("\nè¶…é™æ–‡æ¡£æ˜ç»†ï¼ˆå‰10æ¡ï¼‰:")
#     for doc in over_limit_docs[:10]:
#         print(f"æ–‡æ¡£{doc['æ–‡æ¡£ç¼–å·']:04d} | Tokens: {doc['tokenæ•°é‡']:6d} | æ¥æº: {doc['æ¥æºæ–‡ä»¶']}")
# else:
#     print("\næ²¡æœ‰è¶…è¿‡30526 tokensçš„æ–‡æ¡£")


'''
æœ€å¤§tokenæ•°é‡: 36223
å¯¹åº”æ–‡æ¡£ç¼–å·: 44194

æ€»æ–‡æ¡£æ•°: 60286
æœ‰æ•ˆè®¡ç®—æ–‡æ¡£æ•°: 60286
å¹³å‡tokenæ•°: 1098.4

=== è¶…é™æ–‡æ¡£ç»Ÿè®¡ ===
é˜ˆå€¼è®¾å®š: 30526 tokens
è¶…é™æ–‡æ¡£æ€»æ•°: 2
æ€»è®¡tokenæ•°é‡: 71945

è¶…é™æ–‡æ¡£æ˜ç»†ï¼ˆå‰10æ¡ï¼‰:
æ–‡æ¡£44194 | Tokens:  36223 | æ¥æº: DeltaPrime_exp.sol.json...
æ–‡æ¡£44215 | Tokens:  35722 | æ¥æº: DeltaPrime_exp.sol.json...
'''